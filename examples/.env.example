# =============================================================================
# nfo â€” Environment Configuration
# =============================================================================
# Copy this file to .env and adjust values for your environment.
#
#   cp .env.example .env
#
# nfo's configure() reads these automatically via os.environ.
# Docker Compose loads .env from the project root by default.
# For Python scripts, use python-dotenv: pip install python-dotenv
# =============================================================================

# ---------------------------------------------------------------------------
# Core nfo settings (read by configure())
# ---------------------------------------------------------------------------

# Log level: DEBUG, INFO, WARNING, ERROR
NFO_LEVEL=DEBUG

# Sink specifications (comma-separated)
# Formats: sqlite:<path>, csv:<path>, md:<path>, json:<path>, prometheus:<port>
NFO_SINKS=sqlite:logs/app.db,csv:logs/app.csv

# Logger name (used for stdlib logger bridge)
NFO_NAME=my-app

# ---------------------------------------------------------------------------
# Environment tagging (read by EnvTagger / configure)
# ---------------------------------------------------------------------------

# Environment tag: prod, staging, dev, ci, docker, k8s
# Auto-detected from KUBERNETES_SERVICE_HOST, CI, GITHUB_ACTIONS, etc. if not set
NFO_ENV=dev

# App version (auto-detected from GIT_SHA, APP_VERSION if not set)
NFO_VERSION=1.0.0

# Trace ID for distributed tracing (auto-detected from OTEL_TRACE_ID, etc.)
# NFO_TRACE_ID=

# ---------------------------------------------------------------------------
# LLM-powered log analysis (requires: pip install nfo[llm])
# ---------------------------------------------------------------------------

# litellm model name (e.g. gpt-4o-mini, claude-3-haiku, ollama/llama3)
# NFO_LLM_MODEL=gpt-4o-mini

# OpenAI API key (required for OpenAI models)
# OPENAI_API_KEY=sk-...

# Anthropic API key (required for Claude models)
# ANTHROPIC_API_KEY=sk-ant-...

# Ollama base URL (for local models)
# OLLAMA_BASE_URL=http://localhost:11434

# Enable prompt injection detection in log args
# NFO_DETECT_INJECTION=true

# ---------------------------------------------------------------------------
# HTTP service settings (used by examples/http_service.py)
# ---------------------------------------------------------------------------

# Directory for log files (SQLite, CSV, JSONL)
NFO_LOG_DIR=./logs

# HTTP service host and port
NFO_HOST=0.0.0.0
NFO_PORT=8080

# ---------------------------------------------------------------------------
# Webhook alerts (Slack / Discord / Teams)
# ---------------------------------------------------------------------------

# Webhook URL for ERROR alerts
# NFO_WEBHOOK_URL=https://hooks.slack.com/services/T.../B.../xxx

# Webhook format: slack, discord, teams, raw
# NFO_WEBHOOK_FORMAT=slack

# ---------------------------------------------------------------------------
# Prometheus metrics (requires: pip install nfo[prometheus])
# ---------------------------------------------------------------------------

# Prometheus client HTTP port (for /metrics endpoint)
NFO_PROMETHEUS_PORT=9090

# ---------------------------------------------------------------------------
# Docker / CI auto-detection hints
# These are typically set by your orchestrator, not manually.
# ---------------------------------------------------------------------------

# KUBERNETES_SERVICE_HOST=10.0.0.1
# DOCKER_CONTAINER=true
# CI=true
# GITHUB_ACTIONS=true
# GITLAB_CI=true
# GIT_SHA=abc123def456
# APP_VERSION=1.2.3
# TRACE_ID=req-abc-123
# OTEL_TRACE_ID=4bf92f3577b34da6a3ce929d0e0e4736
